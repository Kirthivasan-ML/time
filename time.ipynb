{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OykI7tvvm9gK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Advanced Time Series Forecasting with Deep Learning (LSTM + Attention / Transformer)\n",
        "\n",
        "Single-file, production-ready Python script that:\n",
        "1) Generates a synthetic multivariate time series dataset (>=1000 obs) with seasonality, trend, regime shifts\n",
        "2) Preprocesses data and creates rolling-origin cross-validation folds\n",
        "3) Implements two deep-learning models:\n",
        "   - LSTM encoder + Bahdanau attention (PyTorch)\n",
        "   - Transformer encoder (PyTorch)\n",
        "4) Benchmarks against a statistical SARIMA model (statsmodels)\n",
        "5) Trains, evaluates (RMSE, MAE, MAPE), and visualizes results and attention weights\n",
        "6) Exports a simple plain-text technical report summarizing methodology and results\n",
        "\n",
        "Dependencies (use the requirements.txt below):\n",
        "- numpy\n",
        "- pandas\n",
        "- scikit-learn\n",
        "- matplotlib\n",
        "- seaborn\n",
        "- torch\n",
        "- tqdm\n",
        "- statsmodels\n",
        "- scipy\n",
        "\n",
        "Usage:\n",
        "python advanced_time_series_attention_project.py\n",
        "\n",
        "Adjust hyperparameters near the top of the file.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# -----------------------------\n",
        "# Config / hyperparameters\n",
        "# -----------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "N_SAMPLES = 4000        # number of time steps (>=1000)\n",
        "N_FEATURES = 4         # number of covariates (including target)\n",
        "LOOKBACK = 60          # input window length\n",
        "HORIZON = 7            # forecast horizon (multi-step)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "OUTPUT_DIR = 'output_ts_project'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities and metrics\n",
        "# -----------------------------\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    # avoid division by zero\n",
        "    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, np.abs(y_true))\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
        "\n",
        "# -----------------------------\n",
        "# Synthetic dataset generator\n",
        "# -----------------------------\n",
        "\n",
        "def generate_synthetic_multivariate(n_steps: int, n_features: int, seed: int = SEED) -> pd.DataFrame:\n",
        "    \"\"\"Generate a multivariate time series with seasonality, trend, interactions, noise, and regime shifts.\n",
        "    First column is the target variable named 'y'; others are covariates x1, x2, ...\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    # components for target\n",
        "    trend = 0.01 * t\n",
        "    seasonal = 2.0 * np.sin(2 * np.pi * t / 24)  # daily-like seasonality\n",
        "    longer_season = 0.5 * np.sin(2 * np.pi * t / 168)  # weekly-like\n",
        "\n",
        "    # regime shifts: at two points amplitude changes\n",
        "    regime = np.ones(n_steps)\n",
        "    regime[t > int(0.5 * n_steps)] = 1.6\n",
        "    regime[t > int(0.8 * n_steps)] = 0.6\n",
        "\n",
        "    # covariates\n",
        "    data = np.zeros((n_steps, n_features))\n",
        "    # target is influenced by covariates\n",
        "    x1 = 0.5 * np.sin(2 * np.pi * t / 12 + 0.2) + 0.1 * np.random.randn(n_steps)\n",
        "    x2 = 0.3 * np.cos(2 * np.pi * t / 24 * 0.6) + 0.05 * np.random.randn(n_steps)\n",
        "    x3 = (np.sign(np.sin(2 * np.pi * t / 200)) + 1) * 0.5 + 0.05 * np.random.randn(n_steps)  # slow regime-like\n",
        "\n",
        "    noise = 0.3 * np.random.randn(n_steps)\n",
        "\n",
        "    y = (trend + seasonal + longer_season) * regime + 0.7 * x1 + 0.4 * x2 + 0.2 * x3 + noise\n",
        "\n",
        "    data[:,0] = y\n",
        "    covs = [x1, x2, x3]\n",
        "    for i in range(1, n_features):\n",
        "        if i-1 < len(covs):\n",
        "            data[:, i] = covs[i-1]\n",
        "        else:\n",
        "            data[:, i] = 0.1 * np.random.randn(n_steps)\n",
        "\n",
        "    cols = ['y'] + [f'x{i}' for i in range(1, n_features)]\n",
        "    df = pd.DataFrame(data, columns=cols)\n",
        "    df['t'] = pd.date_range(start='2000-01-01', periods=n_steps, freq='H')\n",
        "    df = df.set_index('t')\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset and DataLoader\n",
        "# -----------------------------\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data: np.ndarray, lookback: int, horizon: int):\n",
        "        # data: (T, features)\n",
        "        self.data = data\n",
        "        self.lookback = lookback\n",
        "        self.horizon = horizon\n",
        "        self.T = data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, self.T - self.lookback - self.horizon + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx\n",
        "        end = idx + self.lookback\n",
        "        x = self.data[start:end]  # (lookback, features)\n",
        "        y = self.data[end:end + self.horizon, 0]  # target only, horizon steps\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# Models\n",
        "# -----------------------------\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_dim, dec_dim):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(enc_dim, dec_dim)\n",
        "        self.W2 = nn.Linear(dec_dim, dec_dim)\n",
        "        self.V = nn.Linear(dec_dim, 1)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_hidden):\n",
        "        # encoder_outputs: (batch, seq_len, enc_dim)\n",
        "        # decoder_hidden: (batch, dec_dim)  -- we'll use a projection of last hidden\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "        dec_hidden_expanded = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch, seq_len, dec_dim)\n",
        "        score = torch.tanh(self.W1(encoder_outputs) + self.W2(dec_hidden_expanded))\n",
        "        attention_weights = torch.softmax(self.V(score).squeeze(-1), dim=1)  # (batch, seq_len)\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (batch, enc_dim)\n",
        "        return context, attention_weights\n",
        "\n",
        "class LSTMWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, n_layers=1, dropout=0.1, horizon=1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "        self.attention = BahdanauAttention(enc_dim=hidden_dim, dec_dim=hidden_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, features)\n",
        "        enc_out, (h_n, c_n) = self.encoder(x)\n",
        "        # take last layer hidden state\n",
        "        last_hidden = h_n[-1]  # (batch, hidden_dim)\n",
        "        context, attn_weights = self.attention(enc_out, last_hidden)\n",
        "        out = self.fc(context)  # (batch, horizon)\n",
        "        return out, attn_weights\n",
        "\n",
        "# Simple Transformer encoder model for forecasting\n",
        "class TransformerForecaster(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, horizon=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(nn.Linear(d_model, dim_feedforward), nn.ReLU(), nn.Linear(dim_feedforward, horizon))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, features)\n",
        "        z = self.input_proj(x)  # (batch, seq_len, d_model)\n",
        "        enc = self.transformer(z)  # (batch, seq_len, d_model)\n",
        "        # pool over time\n",
        "        pooled = enc.mean(dim=1)  # (batch, d_model)\n",
        "        out = self.fc(pooled)\n",
        "        # no attention weights produced\n",
        "        return out, None\n",
        "\n",
        "# -----------------------------\n",
        "# Training & evaluation functions\n",
        "# -----------------------------\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        preds, _ = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds_all = []\n",
        "    y_all = []\n",
        "    attn_weights_all = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            preds, attn = model(x)\n",
        "            loss = criterion(preds, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            preds_all.append(preds.cpu().numpy())\n",
        "            y_all.append(y.cpu().numpy())\n",
        "            if attn is not None:\n",
        "                attn_weights_all.append(attn.cpu().numpy())\n",
        "    preds_all = np.vstack(preds_all)\n",
        "    y_all = np.vstack(y_all)\n",
        "    if attn_weights_all:\n",
        "        attn_weights_all = np.vstack(attn_weights_all)\n",
        "    else:\n",
        "        attn_weights_all = None\n",
        "    return total_loss / len(dataloader.dataset), preds_all, y_all, attn_weights_all\n",
        "\n",
        "# -----------------------------\n",
        "# Rolling-origin cross validation\n",
        "# -----------------------------\n",
        "\n",
        "def rolling_origin_splits(data_len, initial_train_size, horizon, step):\n",
        "    \"\"\"Yield (train_indices, val_indices) pairs for rolling-origin evaluation.\n",
        "    train window grows by `step` each fold (expanding window).\"\"\"\n",
        "    start = initial_train_size\n",
        "    while start + horizon <= data_len:\n",
        "        train_idx = np.arange(0, start)\n",
        "        val_idx = np.arange(start, min(start + step, data_len - horizon + 1))  # produce multiple validation starts\n",
        "        # We'll generate samples from these indices when building datasets\n",
        "        yield train_idx, val_idx\n",
        "        start += step\n",
        "\n",
        "# -----------------------------\n",
        "# SARIMA benchmark function\n",
        "# -----------------------------\n",
        "\n",
        "def sarima_forecast(train_series: pd.Series, val_index: pd.DatetimeIndex, horizon: int):\n",
        "    # use a simple SARIMAX with seasonal order guess (24) for hourly-seasonality; keep it small for speed\n",
        "    try:\n",
        "        model = sm.tsa.SARIMAX(train_series, order=(1,1,1), seasonal_order=(1,0,1,24), enforce_stationarity=False, enforce_invertibility=False)\n",
        "        res = model.fit(disp=False)\n",
        "        preds = res.predict(start=val_index[0], end=val_index[0] + pd.Timedelta(hours=horizon-1))\n",
        "        return preds.values\n",
        "    except Exception as e:\n",
        "        print('SARIMA failed:', e)\n",
        "        # fallback: naive persistence\n",
        "        last = train_series.iloc[-1]\n",
        "        return np.ones(horizon) * last\n",
        "\n",
        "# -----------------------------\n",
        "# Main pipeline\n",
        "# -----------------------------\n",
        "\n",
        "def prepare_data(df: pd.DataFrame, feature_cols: List[str]):\n",
        "    # scaler fit on training range externally\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(df[feature_cols])\n",
        "    return scaled, scaler\n",
        "\n",
        "\n",
        "def run_pipeline():\n",
        "    # 1) Data generation\n",
        "    df = generate_synthetic_multivariate(N_SAMPLES, N_FEATURES)\n",
        "    df.to_csv(os.path.join(OUTPUT_DIR, 'synthetic_data.csv'))\n",
        "\n",
        "    feature_cols = df.columns.tolist()  # includes 'y' and x1..xn\n",
        "\n",
        "    # We'll run rolling-origin CV with expanding window\n",
        "    initial_train = int(0.5 * N_SAMPLES)\n",
        "    step = int(0.1 * N_SAMPLES)\n",
        "\n",
        "    all_results = []\n",
        "    fold_id = 0\n",
        "\n",
        "    for train_idx, val_idx in rolling_origin_splits(len(df), initial_train, HORIZON, step):\n",
        "        fold_id += 1\n",
        "        print(f\"Running fold {fold_id} — train till {train_idx[-1] if len(train_idx)>0 else 0}, val starts at {val_idx[0]}\")\n",
        "\n",
        "        # build training and validation slices\n",
        "        # For modeling we need arrays; we will fit scaler on train only\n",
        "        train_df = df.iloc[train_idx]\n",
        "        val_start = val_idx[0]\n",
        "        val_end = val_start + step + HORIZON - 1\n",
        "        val_df = df.iloc[val_start: val_end]\n",
        "\n",
        "        # scale\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(train_df[feature_cols])\n",
        "        train_scaled = scaler.transform(train_df[feature_cols])\n",
        "        val_scaled = scaler.transform(val_df[feature_cols])\n",
        "\n",
        "        # Datasets\n",
        "        train_dataset = TimeSeriesDataset(train_scaled, LOOKBACK, HORIZON)\n",
        "        val_dataset = TimeSeriesDataset(np.vstack([train_scaled[-LOOKBACK:], val_scaled]), LOOKBACK, HORIZON)\n",
        "        # note: for validation we append last LOOKBACK from train to the new val window to allow first samples\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # instantiate models\n",
        "        input_dim = train_scaled.shape[1]\n",
        "        lstm_model = LSTMWithAttention(input_dim, hidden_dim=64, n_layers=1, horizon=HORIZON).to(DEVICE)\n",
        "        trans_model = TransformerForecaster(input_dim, d_model=64, nhead=4, num_layers=2, horizon=HORIZON).to(DEVICE)\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Train LSTM with attention\n",
        "        optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
        "        best_val_loss = float('inf')\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss = train_epoch(lstm_model, train_loader, optimizer, criterion)\n",
        "            val_loss, preds_val, y_val, attn_val = evaluate_model(lstm_model, val_loader, criterion)\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(lstm_model.state_dict(), os.path.join(OUTPUT_DIR, f'lstm_fold{fold_id}.pt'))\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"[LSTM] Fold {fold_id} Epoch {epoch}: train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
        "\n",
        "        # Evaluate saved best model on val set\n",
        "        lstm_model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, f'lstm_fold{fold_id}.pt')))\n",
        "        _, preds_val_lstm, y_val_lstm, attn_val = evaluate_model(lstm_model, val_loader, criterion)\n",
        "\n",
        "        # Flatten multi-step predictions for metrics (evaluate per-horizon aggregated)\n",
        "        # We'll compute metrics for horizon=1..H on aggregated predictions\n",
        "        results = {'fold': fold_id, 'lstm_preds': preds_val_lstm, 'y_val': y_val_lstm}\n",
        "\n",
        "        # Train Transformer\n",
        "        optimizer_t = torch.optim.Adam(trans_model.parameters(), lr=LR)\n",
        "        best_val_loss_t = float('inf')\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss_t = train_epoch(trans_model, train_loader, optimizer_t, criterion)\n",
        "            val_loss_t, preds_val_t, y_val_t_t, _ = evaluate_model(trans_model, val_loader, criterion)\n",
        "            if val_loss_t < best_val_loss_t:\n",
        "                best_val_loss_t = val_loss_t\n",
        "                torch.save(trans_model.state_dict(), os.path.join(OUTPUT_DIR, f'trans_fold{fold_id}.pt'))\n",
        "            if epoch % 5 == 0:\n",
        "                print(f\"[Transformer] Fold {fold_id} Epoch {epoch}: train_loss={train_loss_t:.4f} val_loss={val_loss_t:.4f}\")\n",
        "\n",
        "        trans_model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR, f'trans_fold{fold_id}.pt')))\n",
        "        _, preds_val_trans, y_val_trans, _ = evaluate_model(trans_model, val_loader, criterion)\n",
        "        results.update({'trans_preds': preds_val_trans})\n",
        "\n",
        "        # SARIMA baseline: create series combining train and val for indexing\n",
        "        # We'll forecast horizon starting at val start time\n",
        "        train_series = train_df['y']\n",
        "        # get the exact timestamps for val horizon\n",
        "        val_index = df.index[val_start: val_start + HORIZON]\n",
        "        sarima_preds = sarima_forecast(train_series, val_index, HORIZON)\n",
        "        results.update({'sarima_preds': sarima_preds})\n",
        "\n",
        "        # compute metrics aggregated across all validation samples\n",
        "        # preds arrays have shape (n_samples_val, horizon)\n",
        "        def compute_metrics(y_true_arr, y_pred_arr):\n",
        "            # compute per-horizon metrics and overall flattened metrics\n",
        "            n_samples = y_true_arr.shape[0]\n",
        "            per_h_metrics = []\n",
        "            for h in range(y_true_arr.shape[1]):\n",
        "                ytrue = y_true_arr[:, h]\n",
        "                ypred = y_pred_arr[:, h]\n",
        "                per_h_metrics.append({'rmse': rmse(ytrue, ypred), 'mae': mean_absolute_error(ytrue, ypred), 'mape': mape(ytrue, ypred)})\n",
        "            # flattened\n",
        "            flat_rmse = rmse(y_true_arr.flatten(), y_pred_arr.flatten())\n",
        "            flat_mae = mean_absolute_error(y_true_arr.flatten(), y_pred_arr.flatten())\n",
        "            flat_mape = mape(y_true_arr.flatten(), y_pred_arr.flatten())\n",
        "            return {'per_h': per_h_metrics, 'flat': {'rmse': flat_rmse, 'mae': flat_mae, 'mape': flat_mape}}\n",
        "\n",
        "        metrics_lstm = compute_metrics(y_val_lstm, preds_val_lstm)\n",
        "        metrics_trans = compute_metrics(y_val_trans, preds_val_trans)\n",
        "\n",
        "        # For SARIMA, we only have single forecast per fold; expand to match shape\n",
        "        # We'll compare SARIMA to first validation window only (simple approach)\n",
        "        # create repeated rows to compare with sample count\n",
        "        n_val_samples = preds_val_lstm.shape[0]\n",
        "        sarima_rep = np.tile(sarima_preds.reshape(1, -1), (n_val_samples, 1))\n",
        "        metrics_sarima = compute_metrics(y_val_lstm, sarima_rep)\n",
        "\n",
        "        results.update({'metrics_lstm': metrics_lstm, 'metrics_transformer': metrics_trans, 'metrics_sarima': metrics_sarima, 'attn_weights': attn_val})\n",
        "        all_results.append(results)\n",
        "\n",
        "        # Save intermediate results\n",
        "        with open(os.path.join(OUTPUT_DIR, f'results_fold{fold_id}.json'), 'w') as f:\n",
        "            json.dump({'metrics_lstm': metrics_lstm, 'metrics_transformer': metrics_trans, 'metrics_sarima': metrics_sarima}, f, indent=2)\n",
        "\n",
        "        # quick plot for fold\n",
        "        plot_fold_results(df, scaler, val_start, LOOKBACK, preds_val_lstm, preds_val_trans, sarima_preds, y_val_lstm, attn_val, fold_id)\n",
        "\n",
        "    # Summarize across folds\n",
        "    save_aggregate_report(all_results)\n",
        "\n",
        "# -----------------------------\n",
        "# Plotting helper\n",
        "# -----------------------------\n",
        "\n",
        "def plot_fold_results(df, scaler, val_start, lookback, preds_lstm, preds_trans, sarima_preds, y_val, attn_weights, fold_id):\n",
        "    # plot first validation sample actual vs predicted for horizon\n",
        "    plt.figure(figsize=(12,6))\n",
        "    # pick a representative sample (first)\n",
        "    sample_idx = 0\n",
        "    target_series = df['y'].values\n",
        "    t_indices = np.arange(val_start, val_start + lookback + HORIZON)\n",
        "\n",
        "    # reconstruct last lookback values (unscale)\n",
        "    # we need scaler to invert: scaler.mean_ and scale_\n",
        "    # Build a small helper to inverse-transform single-sample arrays\n",
        "    def inv_scale_window(window_scaled):\n",
        "        # window_scaled: (L, features)\n",
        "        arr = np.array(window_scaled)\n",
        "        mean = scaler.mean_\n",
        "        scale = scaler.scale_\n",
        "        return arr * scale + mean\n",
        "\n",
        "    # reconstruct the ground truth window\n",
        "    window_start = val_start - lookback\n",
        "    raw_window = df.iloc[window_start: val_start + HORIZON]\n",
        "    x_times = raw_window.index\n",
        "\n",
        "    # plot true y\n",
        "    y_true = raw_window['y'].values\n",
        "    plt.plot(x_times, y_true, label='Ground truth')\n",
        "\n",
        "    # predicted sequences start at val_start\n",
        "    pred_times = df.index[val_start: val_start + HORIZON]\n",
        "    # LSTM preds\n",
        "    ypred_lstm = preds_lstm[sample_idx]\n",
        "    ypred_trans = preds_trans[sample_idx]\n",
        "    plt.plot(pred_times, ypred_lstm, marker='o', linestyle='--', label='LSTM+Attn')\n",
        "    plt.plot(pred_times, ypred_trans, marker='x', linestyle='--', label='Transformer')\n",
        "    plt.plot(pred_times, sarima_preds, marker='s', linestyle='--', label='SARIMA')\n",
        "\n",
        "    plt.title(f'Fold {fold_id} — example forecast (horizon={HORIZON})')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f'fold{fold_id}_forecast.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # attention heatmap for the example sample (if available)\n",
        "    if attn_weights is not None:\n",
        "        attn_sample = attn_weights[sample_idx]  # (seq_len,)\n",
        "        plt.figure(figsize=(10,2))\n",
        "        sns.heatmap(attn_sample.reshape(1, -1), cmap='viridis', cbar=True, xticklabels=False)\n",
        "        plt.title(f'Fold {fold_id} — Attention weights (example sample)')\n",
        "        plt.xlabel('Encoder time step')\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f'fold{fold_id}_attention.png'))\n",
        "        plt.close()\n",
        "\n",
        "# -----------------------------\n",
        "# Report generation\n",
        "# -----------------------------\n",
        "\n",
        "def save_aggregate_report(all_results):\n",
        "    # generate a small plain-text technical report summarizing metrics\n",
        "    lines = []\n",
        "    lines.append('Technical report — Time Series Forecasting with Attention')\n",
        "    lines.append('\\nSummary of folds:\\n')\n",
        "    for r in all_results:\n",
        "        lines.append(f\"Fold {r['fold']} — LSTM flat RMSE: {r['metrics_lstm']['flat']['rmse']:.4f}, Transformer flat RMSE: {r['metrics_transformer']['flat']['rmse']:.4f}, SARIMA flat RMSE: {r['metrics_sarima']['flat']['rmse']:.4f}\")\n",
        "    lines.append('\\nNotes:')\n",
        "    lines.append('- Models trained with PyTorch. LSTM model includes Bahdanau-style attention to provide interpretability (attention weights).')\n",
        "    lines.append('- Rolling-origin (expanding window) evaluation used across multiple folds.')\n",
        "    lines.append('- Metrics reported: RMSE, MAE, MAPE (per-horizon and flattened).')\n",
        "    report_text = '\\n'.join(lines)\n",
        "    with open(os.path.join(OUTPUT_DIR, 'technical_report.txt'), 'w') as f:\n",
        "        f.write(report_text)\n",
        "    print('Saved technical report to', os.path.join(OUTPUT_DIR, 'technical_report.txt'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_pipeline()\n",
        "\n"
      ]
    }
  ]
}